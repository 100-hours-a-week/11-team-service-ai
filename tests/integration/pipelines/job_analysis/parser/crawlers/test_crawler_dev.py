import logging
import os
from pipelines.job_analysis.infrastructure.adapters.crawling.router import (
    DynamicRoutingCrawler,
)

# ë¡œê·¸ ì„¤ì •
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger("DevTester")


def test_crawlers():
    # í…ŒìŠ¤íŠ¸í•  URL ëª©ë¡
    test_urls = [
            "https://www.wanted.co.kr/wd/330563",
    ]

    # ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
    output_dir = "./tests/integration/pipelines/job_analysis/parser/data/"
    os.makedirs(output_dir, exist_ok=True)

    print("\n" + "=" * 60)
    print("ğŸš€ Starting Development Crawler Test")
    print("=" * 60 + "\n")

    for url in test_urls:

        print(f"ğŸ¯ Testing [{url}]")

        try:
            # 1. ë¼ìš°íŒ… í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
            crawler = DynamicRoutingCrawler()

            # (ì˜µì…˜) ë‚´ë¶€ ì „ëµ í™•ì¸ì„ ìœ„í•œ ë¡œê¹…ì€ router ë‚´ë¶€ êµ¬í˜„ì— ë”°ë¦„
            # ì—¬ê¸°ì„œëŠ” fetch í˜¸ì¶œë§Œ í•˜ë©´ ë¨

            # 2. í¬ë¡¤ë§ ì‹¤í–‰
            text = crawler.fetch(url)

            # 3. ê²°ê³¼ ê²€ì¦
            content_length = len(text)
            print(f"âœ… Success! Content Length: {content_length} chars")

            if content_length < 50:
                print("âš ï¸  Warning: Content seems too short!")

            # 4. íŒŒì¼ ì €ì¥
            filename = f"crawlers_result.txt"
            filepath = os.path.join(output_dir, filename)
            with open(filepath, "w", encoding="utf-8") as f:
                f.write(text)

            print(f"ğŸ’¾ Saved sample to: {filepath}")

            # 5. ë³¸ë¬¸ ë¯¸ë¦¬ë³´ê¸° (ì• 200ì)
            print("-" * 20 + " Preview " + "-" * 20)
            print(text[:200].replace("\n", " ") + "...")
            print("-" * 50)

        except Exception as e:
            print(f"âŒ Failed: {e}")

        print("\n" + "=" * 60 + "\n")


if __name__ == "__main__":
    test_crawlers()
