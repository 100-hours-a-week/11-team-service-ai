"""Shared utility functions used in the project.

Functions:
    load_chat_model: Load a chat model based on provider and name.
"""

import logging

from langchain_core.language_models import BaseChatModel
from shared.config import settings
from pydantic import BaseModel, Field

logger = logging.getLogger(__name__)


class AiResponse(BaseModel):
    """ì‘ë‹µê²°ê³¼ ì •í˜•í™”"""

    response: str = Field(description="ì‘ë‹µê²°ê³¼")


def load_chat_model(model_name: str, model_provider: str) -> BaseChatModel:
    """Load a chat model based on the provider and name."""

    if model_provider == "gemini":
        from langchain_google_genai import ChatGoogleGenerativeAI

        logger.info(f"ğŸ¤– Loading Gemini Model: {model_name}")
        return ChatGoogleGenerativeAI(
            model=model_name,
            google_api_key=settings.GOOGLE_API_KEY,
            temperature=0,
        )
    elif model_provider == "vllm":
        from langchain_openai import ChatOpenAI
        
        logger.info(f"ğŸ¤– Loading vLLM Model: {model_name}")

        return ChatOpenAI(
            model=model_name,
            api_key="EMPTY",  # vLLMì€ ê¸°ë³¸ì ìœ¼ë¡œ api keyë¥¼ ìš”êµ¬í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ë”ë¯¸ê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
            base_url=settings.VLLM_BASE_URL,
            temperature=0,
            # max_tokens=1024, # í•„ìš”ì— ë”°ë¼ ì„¤ì • 
        )
    else:  # openai or default
        from langchain_openai import ChatOpenAI
        from pydantic import SecretStr

        logger.info(f"ğŸ¤– Loading OpenAI Model: {model_name}")
        return ChatOpenAI(
            model=model_name,
            temperature=0,
            api_key=(
                SecretStr(settings.OPENAI_API_KEY) if settings.OPENAI_API_KEY else None
            ),
        )
