from typing import Any
import logging
import asyncio
from job_analysis.parser.crawlers.factory import CrawlerFactory
from job_analysis.parser.extract.extractor import JobPostingExtractor

logger = logging.getLogger(__name__)

class SimpleJobExtractionService:
    """
    ë‹¨ìˆœ í¬ë¡¤ë§ ë° ë°ì´í„° ì¶”ì¶œë§Œ ìˆ˜í–‰í•˜ëŠ” ì„œë¹„ìŠ¤
    DB ì €ì¥ì´ë‚˜ ì¤‘ë³µ ì²´í¬ ì—†ì´ ìˆœìˆ˜í•˜ê²Œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì—­í• 
    """
    def __init__(self):
        self.extractor = JobPostingExtractor()

    async def extract_from_url(self, url: str) -> Any:
        """
        URL -> í¬ë¡¤ë§ -> ì¶”ì¶œ -> Dictionary(Pydantic Model) ë°˜í™˜
        """
        # 1. í¬ë¡¤ë§ (Crawling)
        raw_text = await self._crawl_content(url)

        # 2. ì¶”ì¶œ (Extraction)
        extracted_data = await self._extract_data(raw_text)

        # 3. Response ë§¤í•‘ (DB ì €ì¥ì´ ì—†ìœ¼ë¯€ë¡œ IDëŠ” ì„ì‹œê°’ 0 ì‚¬ìš©)
        from shared.schema.job_posting import JobPostingAnalyzeResponse, RecruitmentPeriod

        recruitment_period = None
        if extracted_data.start_date or extracted_data.end_date:
            recruitment_period = RecruitmentPeriod(
                start_date=extracted_data.start_date,
                end_date=extracted_data.end_date
            )

        return JobPostingAnalyzeResponse(
            job_posting_id=0,  # ì €ì¥ë˜ì§€ ì•ŠìŒ
            is_existing=False,
            company_name=extracted_data.company_name,
            job_title=extracted_data.job_title,
            main_responsibilities=extracted_data.main_tasks if isinstance(extracted_data.main_tasks, list) else [],
            required_skills=extracted_data.tech_stacks if isinstance(extracted_data.tech_stacks, list) else [],
            recruitment_status="OPEN", # ê¸°ë³¸ê°’
            recruitment_period=recruitment_period,
            ai_summary=extracted_data.ai_summary or "",
            evaluation_criteria=[item.model_dump() for item in extracted_data.evaluation_criteria] if extracted_data.evaluation_criteria else []
        )

    async def _crawl_content(self, url: str) -> str:
        """URLì—ì„œ í…ìŠ¤íŠ¸ ì½˜í…ì¸ ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤."""
        logger.info(f"ğŸŒ Crawling URL: {url}")
        try:
            crawler = CrawlerFactory.get_crawler(url)
            # PlaywrightëŠ” ë¸”ë¡œí‚¹ I/Oì´ë¯€ë¡œ ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
            raw_text = await asyncio.to_thread(crawler.fetch, url)

            if not raw_text or len(raw_text) < 50:
                raise ValueError("Crawled content is empty or too short.")

            logger.info(f"âœ… Crawling successful. Length: {len(raw_text)} chars")
            return raw_text
        except ValueError as e:
            logger.error(f"âŒ Crawling validation failed: {e}")
            raise
        except Exception as e:
            logger.error(f"âŒ Crawling failed: {e}", exc_info=True)
            raise RuntimeError(f"Crawling failed: {e}") from e

    async def _extract_data(self, raw_text: str):
        """LLMì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ì—ì„œ êµ¬ì¡°í™”ëœ ë°ì´í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        logger.info("ğŸ§  Extracting data using LLM...")
        try:
            extracted_data = await self.extractor.extract(raw_text)
            if not extracted_data:
                raise RuntimeError("LLM Extraction returned empty result")
            logger.info("âœ… Data extraction successful")
            return extracted_data
        except Exception as e:
            logger.error(f"âŒ LLM extraction failed: {e}", exc_info=True)
            if "API" in str(e) or "OpenAI" in str(e):
                raise RuntimeError(f"OpenAI API error: {e}") from e
            raise RuntimeError(f"Data extraction failed: {e}") from e
