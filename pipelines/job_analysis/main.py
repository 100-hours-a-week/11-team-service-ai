import logging
import asyncio

from shared.db.connection import get_db
from shared.schema.job_posting import (
    JobPostingAnalyzeRequest,
    JobPostingAnalyzeResponse,
)
from job_analysis.service import JobAnalysisService

logger = logging.getLogger(__name__)

async def run_pipeline(request: JobPostingAnalyzeRequest) -> JobPostingAnalyzeResponse:
    """
    Job Analysis Pipeline Entrypoint (MSA)
    
    This function handles the entire lifecycle of a job analysis task:
    1. Manages DB session (Self-contained)
    2. Orchestrates crawling, extraction, and storage via Service
    3. Returns standardized response
    """
    logger.info(f"ğŸš€ [Pipeline Start] Job Analysis for URL: {request.url}")
    
    try:
        # DB ì„¸ì…˜ ìƒì„± (Context Managerë¡œ ìë™ ê´€ë¦¬)
        async for session in get_db():
            service = JobAnalysisService(session)
            
            # ì„œë¹„ìŠ¤ ì‹¤í–‰ (ì´ì œ Pydantic Modelì´ ë°˜í™˜ë¨)
            response = await service.run_analysis(request.url)
            
            logger.info(f"âœ… [Pipeline Success] Job ID: {response.job_posting_id}")
            return response

    except Exception as e:
        logger.error(f"âŒ [Pipeline Failed] Error: {e}", exc_info=True)
        # ì—ëŸ¬ ë°œìƒ ì‹œì—ë„ ì ì ˆí•œ Error Responseë¥¼ ë°˜í™˜í•˜ê±°ë‚˜ raise
        raise e

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰ìš© (CLI)
if __name__ == "__main__":
    import asyncio
    
    async def main():
        req = JobPostingAnalyzeRequest(url="https://www.saramin.co.kr/zf_user/jobs/relay/view?rec_idx=49195689")
        res = await run_pipeline(req)
        print("Result:", res)

    asyncio.run(main())
