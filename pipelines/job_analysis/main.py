import logging

from shared.schema.job_posting import (
    JobPostingAnalyzeRequest,
    JobPostingAnalyzeResponse,
    JobPostingDeleteResponse,
)
from .application.services.extraction_service import JobExtractionService
from .infrastructure.adapters.crawling.router import DynamicRoutingCrawler
from .infrastructure.adapters.llm.job_extractor import LLMJobExtractor

from shared.config import settings
from .infrastructure.adapters.llm.mock_extractor import MockJobExtractor

logger = logging.getLogger(__name__)


# TODO: ê³µê³ ë¶„ì„ íŒŒì´í”„ë¼ì¸ êµ¬í˜„, ë²¡í„°dbì—ë§Œ ê³µê³  ì €ìž¥
async def run_pipeline(request: JobPostingAnalyzeRequest) -> JobPostingAnalyzeResponse:
    """
    í¬ë¡¤ë§ ë° ì¶”ì¶œ íŒŒì´í”„ë¼ì¸
    """
    # ì„¤ì •ì— ë”°ë¼ Extractor ì£¼ìž… ê²°ì •
    extractor_impl = None
    if settings.use_mock:
        extractor_impl = MockJobExtractor()
    else:
        llm_model = None
        if getattr(settings, "LLM_PROVIDER", "openai") == "gemini":
            from langchain_google_genai import ChatGoogleGenerativeAI

            model = getattr(settings, "GOOGLE_MODEL", "gemini-3-flash-preview")
            logger.info(f"ðŸ¤– Initializing LLMJobExtractor with gemini ({model})")

            llm_model = ChatGoogleGenerativeAI(
                model=model,
                google_api_key=settings.GOOGLE_API_KEY,
                temperature=0,
            )
        else:
            from langchain_openai import ChatOpenAI
            from pydantic import SecretStr

            model = getattr(settings, "OPENAI_MODEL", "gpt-4o-mini")
            logger.info(f"ðŸ¤– Initializing LLMJobExtractor with OpenAI ({model})")

            llm_model = ChatOpenAI(
                model=model,
                temperature=0,
                api_key=(
                    SecretStr(settings.OPENAI_API_KEY)
                    if settings.OPENAI_API_KEY
                    else None
                ),
                model_kwargs={"response_format": {"type": "json_object"}},
            )
        
        extractor_impl = LLMJobExtractor(llm=llm_model)

    service = JobExtractionService(
        crawler=DynamicRoutingCrawler(), extractor=extractor_impl
    )
    return await service.extract_job_data(request.url)


# TODO: ì‚­ì œ íŒŒì´í”„ë¼ì¸ êµ¬í˜„, ë²¡í„°dbì— ì €ìž¥ëœ ë‚´ìš©ë§Œ ì‚­ì œ
async def delete_pipeline(job_posting_id: int) -> JobPostingDeleteResponse:
    """
    Job Posting Deletion Pipeline Entrypoint
    """
    logger.info(f"ðŸš€ [Pipeline Start] Delete Job Posting ID: {job_posting_id}")
    return JobPostingDeleteResponse(deleted_id=job_posting_id)
