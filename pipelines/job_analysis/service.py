from typing import Optional, Any
import logging
import hashlib
from datetime import datetime
from sqlalchemy.ext.asyncio import AsyncSession

from job_analysis.data.repository.job_post_repository import JobPostRepository
from job_analysis.data.repository.job_master_repository import JobMasterRepository
from job_analysis.data.repository.company_repository import CompanyRepository
from job_analysis.data.repository.skill_repository import SkillRepository
from job_analysis.data.repository.job_posting_query_repository import JobPostingQueryRepository
from job_analysis.data.repository.dto import JobPostingWithRelations
from job_analysis.data.models import JobPost, JobMaster
from job_analysis.parser.crawlers.factory import CrawlerFactory
from job_analysis.parser.extract.extractor import JobPostingExtractor
from job_analysis.utils.fingerprint import FingerprintGenerator
from shared.schema.job_posting import JobPostingAnalyzeResponse

logger = logging.getLogger(__name__)

class JobAnalysisService:
    def __init__(self, session: AsyncSession):
        self.session = session

        # Repository ì´ˆê¸°í™” (ê° ì—”í‹°í‹°ë³„ + Query ì „ìš©)
        self.job_post_repo = JobPostRepository(session)
        self.job_master_repo = JobMasterRepository(session)
        self.company_repo = CompanyRepository(session)
        self.skill_repo = SkillRepository(session)
        self.query_repo = JobPostingQueryRepository(session)

        self.extractor = JobPostingExtractor() # Extractor ì´ˆê¸°í™”

    def _hash_url(self, url: str) -> str:
        """URL SHA-256 í•´ì‹œ ìƒì„±"""
        return hashlib.sha256(url.encode("utf-8")).hexdigest()

    def _map_to_response(self, post: JobPost, is_existing: bool) -> JobPostingAnalyzeResponse:
        """SQLAlchemy Model -> Pydantic Response ë³€í™˜"""
        # ... (ê¸°ì¡´ ë¡œì§ ìœ ì§€)

    async def run_analysis(self, url: str) -> JobPostingAnalyzeResponse:
        """
        ì±„ìš© ê³µê³  ë¶„ì„ íŒŒì´í”„ë¼ì¸ ë©”ì¸ ì§„ì…ì .
        Returns: JobPostingAnalyzeResponse (Pydantic Model)
        """
        logger.info(f"ğŸš€ Starting Job Analysis for: {url}")

        # 1. URL ì¤‘ë³µ ì²´í¬ (Fast Track)
        existing_result = await self.query_repo.find_with_relations_by_url(url)
        if existing_result:
            logger.info(f"âœ… Found existing job post (ID: {existing_result.job_post.job_post_id}). Returning cached result.")
            return await self._map_to_response_from_dto(existing_result, is_existing=True)

        # 2. í¬ë¡¤ë§ (Crawling)
        raw_text = self._crawl_content(url)

        # 3. ì¶”ì¶œ (Extraction)
        extracted_data = await self._extract_data(raw_text)

        # 4. Fingerprint ìƒì„± ë° ì¤‘ë³µ ì²´í¬
        fingerprint = FingerprintGenerator.generate(
            company_name=extracted_data.company_name,
            job_title=extracted_data.job_title,
            main_tasks=extracted_data.main_tasks
        )
        
        # 5. ê¸°ì¡´ ì½˜í…ì¸  ì¤‘ë³µ ì²˜ë¦¬
        duplicate_response = await self._process_if_duplicate_exists(url, extracted_data, fingerprint)
        if duplicate_response:
            return duplicate_response


        # 6. ì •ê·œí™” ì²˜ë¦¬(íšŒì‚¬ëª…, ìŠ¤í‚¬ì…‹) -> company, company_alias, skills, skills_alias í…Œì´ë¸” í™œìš©
        # ìš°ì„  íšŒì‚¬ë¶€í„° dbì¡°íšŒ -> ì—†ìœ¼ë©´ ë²¡í„°dbì¡°íšŒ -> ìœ ì‚¬ë„ë¥¼ í™•ì¸í•˜ì—¬ ë†’ìŒ, ë‚®ìŒ, ì• ë§¤í•¨ìœ¼ë¡œ êµ¬ë¶„ -> ë†’ìœ¼ë©´ ë°”ë¡œ ë§¤í•‘, ë‚®ìœ¼ë©´ ì‹ ê·œ ìš©ì–´ ë“±ë¡, ì• ë§¤í•˜ë©´ ì—ì´ì „íŠ¸ì—ê²Œ ìš”ì²­

        # ê·¸ë¦¬ê³  ìŠ¤í‚¬ì…‹ ê°ê°ì— ëŒ€í•´ dbì¡°íšŒ -> ì—†ìœ¼ë©´ ë²¡í„°dbì¡°íšŒ -> ìœ ì‚¬ë„ë¥¼ í™•ì¸í•˜ì—¬ ë†’ìŒ, ë‚®ìŒ, ì• ë§¤í•¨ìœ¼ë¡œ êµ¬ë¶„ -> ë†’ìœ¼ë©´ ë°”ë¡œ ë§¤í•‘, ë‚®ìœ¼ë©´ ì‹ ê·œ ìš©ì–´ ë“±ë¡ -> ì• ë§¤í•˜ë©´ ì—ì´ì „íŠ¸ì—ê²Œ ìš”ì²­

        # ë²¡í„°dbëŠ” ì „ì²˜ë¦¬ëœ ë°ì´í„°ê°€ ì¡´ì¬? or ì „ì²˜ë¦¬ë˜ì§€ ì•Šì€ ë°ì´í„° ì¡´ì¬? -> ì „ì²˜ë¦¬ë˜ì§€ ì•Šìœ¼ë©´ ìœ ì‚¬ë„ê°€ ì •í™•í•˜ê²Œ ì¸¡ì • ë¶ˆê°€, ìœ ì‚¬ë„ ì¸¡ì • ì •í™•ë„ë¥¼ ë†’ì´ê¸° ìœ„í•´ ì „ì²˜ë¦¬ í›„ ë²¡í„° dbì— ì €ì¥

        # 6. ì‹ ê·œ ì½˜í…ì¸  ì²˜ë¦¬ (ì™„ì „ ì‹ ê·œ ê³µê³ )
        return await self._process_new_content(url, extracted_data, fingerprint)


    def _crawl_content(self, url: str) -> str:
        """[TODO] URLì—ì„œ í…ìŠ¤íŠ¸ ì½˜í…ì¸ ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤."""
        logger.info("ğŸ†• New URL detected. Proceeding to Crawling...")
        try:
            crawler = CrawlerFactory.get_crawler(url)
            raw_text = crawler.fetch(url)
            
            if not raw_text or len(raw_text) < 50:
                raise ValueError("Crawled content is empty or too short.")
                
            logger.info(f"âœ… Crawling successful. Length: {len(raw_text)} chars")
            return raw_text
        except Exception as e:
            logger.error(f"âŒ Crawling failed: {e}")
            raise RuntimeError(f"Crawling failed: {e}")

    async def _extract_data(self, raw_text: str):
        """ LLMì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ì—ì„œ êµ¬ì¡°í™”ëœ ë°ì´í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        extracted_data = await self.extractor.extract(raw_text)
        if not extracted_data:
            raise RuntimeError("LLM Extraction failed.")
        return extracted_data

    async def _process_if_duplicate_exists(self, url: str, extracted_data: Any, fingerprint: str) -> Optional[JobPostingAnalyzeResponse]:
        """ë‚´ìš© ê¸°ë°˜ ì¤‘ë³µ(Fingerprint)ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  ì²˜ë¦¬í•©ë‹ˆë‹¤. (ìµœì í™”ëœ JOIN ì¿¼ë¦¬)"""

        # ìµœì í™”: í•œ ë²ˆì˜ JOIN ì¿¼ë¦¬ë¡œ ëª¨ë“  ë°ì´í„° ì¡°íšŒ
        # fingerprintì™€ ì¼ì¹˜í•˜ëŠ” ê³µê³ ë¥¼ ì¡°íšŒí•¨
        result = await self.query_repo.find_with_relations_by_fingerprint(fingerprint)

        if result:
            logger.info(f"â™»ï¸ Content Duplicate detected. Linking to existing Master.")

            # ìƒˆ JobPost ìƒì„± (URLì€ ë‹¤ë¥´ì§€ë§Œ ë‚´ìš©ì€ ë™ì¼)
            new_job_post = JobPost(
                job_master_id=result.job_master.job_master_id,
                company_id=result.job_post.company_id,
                source_type="crawler",
                source_url=url,
                source_url_hash=self._hash_url(url),
                raw_company_name=extracted_data.company_name,
                raw_job_title=extracted_data.job_title,
                main_tasks=extracted_data.main_tasks,
                recruitment_status="Open",
                registration_status="Active",
                start_date=None,
                end_date=None,
                created_at=datetime.now(),
                updated_at=datetime.now(),
                fingerprint_hash=fingerprint
            )

            saved_post = await self.job_post_repo.create(new_job_post)

            # DTO ì—…ë°ì´íŠ¸í•˜ì—¬ ì¬ì‚¬ìš©
            result.job_post = saved_post
            return await self._map_to_response_from_dto(result, is_existing=True)

        return None

    async def _map_to_response_from_dto(
        self,
        dto: JobPostingWithRelations,
        is_existing: bool
    ) -> JobPostingAnalyzeResponse:
        """DTO -> Pydantic Response ë³€í™˜ (JobMaster ê¸°ì¤€, ìµœì í™”)"""

        from shared.schema.job_posting import RecruitmentPeriod

        # DTOì—ì„œ ëª¨ë“  ë°ì´í„° ì¶”ì¶œ (ì¶”ê°€ ì¿¼ë¦¬ ì—†ìŒ)
        job_master = dto.job_master
        company = dto.company
        job_post = dto.job_post

        # RecruitmentPeriod ê°ì²´ ìƒì„±
        recruitment_period = RecruitmentPeriod(
            start_date=job_master.start_date,
            end_date=job_master.end_date
        )

        return JobPostingAnalyzeResponse(
            job_posting_id=job_post.job_post_id,
            is_existing=is_existing,
            company_name=company.name,
            job_title=job_master.job_title,
            main_responsibilities=job_master.main_tasks if isinstance(job_master.main_tasks, list) else [],
            required_skills=dto.skills,
            recruitment_status=job_master.status,
            recruitment_period=recruitment_period,
            ai_summary=job_master.ai_summary or ""
        )

    async def _process_new_content(
        self, url: str, extracted_data: Any, fingerprint: str
    ) -> JobPostingAnalyzeResponse:
        """ì™„ì „íˆ ìƒˆë¡œìš´ ì±„ìš© ê³µê³ ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤ (ì‹ ê·œ JobMaster + JobPost ìƒì„±)"""

        logger.info("ğŸ†• Processing completely new job posting.")

        # TODO: 1. Company ìƒì„± ë˜ëŠ” ì¡°íšŒ
        # TODO: 2. JobMaster ìƒì„±
        # TODO: 3. Skills ë§¤ì¹­ ë° ì—°ê²°
        # TODO: 4. JobPost ìƒì„±
        # TODO: 5. Response ë°˜í™˜

        raise NotImplementedError("_process_new_content is not yet implemented")