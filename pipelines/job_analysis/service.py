from typing import Optional, Any
import logging
import hashlib
import asyncio
from sqlalchemy.ext.asyncio import AsyncSession

from job_analysis.data.repository.job_post_repository import JobPostRepository
from job_analysis.data.repository.job_master_repository import JobMasterRepository
from job_analysis.data.repository.company_repository import CompanyRepository
from job_analysis.data.repository.skill_repository import SkillRepository
from job_analysis.data.repository.job_posting_query_repository import (
    JobPostingQueryRepository,
)
from job_analysis.data.repository.dto import JobPostingWithRelations
from job_analysis.data.models import JobPost
from job_analysis.parser.crawlers.factory import CrawlerFactory
from job_analysis.parser.extract.extractor import JobPostingExtractor
from job_analysis.utils.fingerprint import FingerprintGenerator
from shared.schema.job_posting import JobPostingAnalyzeResponse
from job_analysis.normalizer.company_normalizer import CompanyNormalizer
from job_analysis.normalizer.skill_normalizer import SkillNormalizer
from job_analysis.domain.job_registration_service import JobRegistrationService
from job_analysis.domain.job_duplicate_checker import JobDuplicateChecker
from job_analysis.data.vector_repository.job_vector_repo import JobVectorRepository

logger = logging.getLogger(__name__)


class JobAnalysisService:
    def __init__(self, session: AsyncSession):
        self.session = session

        # Repository ì´ˆê¸°í™” (ê° ì—”í‹°í‹°ë³„ + Query ì „ìš©)
        self.job_post_repo = JobPostRepository(session)
        self.job_master_repo = JobMasterRepository(session)
        self.company_repo = CompanyRepository(session)
        self.skill_repo = SkillRepository(session)
        self.query_repo = JobPostingQueryRepository(session)

        # Normalizer ì´ˆê¸°í™” (Repository ì£¼ì…)
        self.company_normalizer = CompanyNormalizer(repo=self.company_repo)
        self.skill_normalizer = SkillNormalizer(repo=self.skill_repo)

        # Vector Repo / Checker ì´ˆê¸°í™”
        self.job_vector_repo = JobVectorRepository()
        self.duplicate_checker = JobDuplicateChecker(
            job_post_repo=self.job_post_repo,
            job_vector_repo=self.job_vector_repo,
            query_repo=self.query_repo,
            skill_repo=self.skill_repo,
        )

        # Domain Service ì´ˆê¸°í™”
        self.registration_service = JobRegistrationService(
            session=self.session,
            job_post_repo=self.job_post_repo,
            job_master_repo=self.job_master_repo,
            company_repo=self.company_repo,
            skill_repo=self.skill_repo,
            company_normalizer=self.company_normalizer,
            skill_normalizer=self.skill_normalizer,
            duplicate_checker=self.duplicate_checker,
            job_vector_repo=self.job_vector_repo,
        )

        self.extractor = JobPostingExtractor()  # Extractor ì´ˆê¸°í™”

    def _hash_url(self, url: str) -> str:
        """URL SHA-256 í•´ì‹œ ìƒì„±"""
        return hashlib.sha256(url.encode("utf-8")).hexdigest()

    def _map_to_response(
        self, post: JobPost, is_existing: bool
    ) -> JobPostingAnalyzeResponse:
        """SQLAlchemy Model -> Pydantic Response ë³€í™˜"""
        # ... (ê¸°ì¡´ ë¡œì§ ìœ ì§€)

    async def run_analysis(self, url: str) -> JobPostingAnalyzeResponse:
        """
        ì±„ìš© ê³µê³  ë¶„ì„ íŒŒì´í”„ë¼ì¸ ë©”ì¸ ì§„ì…ì .
        Returns: JobPostingAnalyzeResponse (Pydantic Model)
        """
        logger.info(f"ğŸš€ Starting Job Analysis for: {url}")

        # 1. URL ì¤‘ë³µ ì²´í¬ (Fast Track via DuplicateChecker)
        existing_result = await self.duplicate_checker.check_existing_post_by_url(url)
        if existing_result:
            logger.info(
                f"âœ… Found existing job post (ID: {existing_result.job_post.job_post_id})"
            )
            return await self._map_to_response_from_dto(
                existing_result, is_existing=True
            )

        # 2. í¬ë¡¤ë§ (Crawling)
        raw_text = await self._crawl_content(url)

        # 3. ì¶”ì¶œ (Extraction)
        extracted_data = await self._extract_data(raw_text)

        # 4. Fingerprint ìƒì„± ë° ì¤‘ë³µ ì²´í¬
        fingerprint = FingerprintGenerator.generate(
            company_name=extracted_data.company_name,
            job_title=extracted_data.job_title,
            main_tasks=extracted_data.main_tasks,
        )
        duplicate_response = await self._handle_content_duplicate(
            url, extracted_data, fingerprint
        )
        if duplicate_response:
            return duplicate_response

        # 5. ì˜ë¯¸ì  ì¤‘ë³µ(Semantic Duplicate) ë° ì •ê·œí™”/ìƒì„±
        #    ì´ ë‹¨ê³„ì—ì„œ íšŒì‚¬ì™€ ìŠ¤í‚¬ì„ DBì— í™•ë³´(Ensure)í•˜ê³  IDë¥¼ ë°›ì•„ì˜µë‹ˆë‹¤.
        #    ë™ì‹œì— extracted_dataë¥¼ ì •ê·œí™”ëœ ë°ì´í„°ë¡œ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤ (normalized_data ë°˜í™˜).

        company_id, skill_ids, normalized_data = (
            await self.registration_service.ensure_and_get_ids(extracted_data)
        )

        # 5-3. í…ìŠ¤íŠ¸ ì§ë ¬í™” (ì „ì²´ ë°ì´í„°ë¥¼ JSONìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ë²¡í„° ê²€ìƒ‰ì— ì‚¬ìš©)
        job_search_text = normalized_data.model_dump_json()

        # 5-4. ì¤‘ë³µ ê²€ì‚¬ ì‹¤í–‰ (JSON í…ìŠ¤íŠ¸ + í™•ë³´ëœ ID ì‚¬ìš©)
        existing_job_master_id = await self.duplicate_checker.check_semantic_duplicate(
            company_id=company_id, job_text=job_search_text
        )

        # 6. ìµœì¢… ë“±ë¡ ì²˜ë¦¬ (ë¶„ê¸°)
        result: JobPostingAnalyzeResponse = None

        if existing_job_master_id:
            logger.info(
                f"â™»ï¸ Semantic Duplicate found! Linking to JobMaster ID: {existing_job_master_id}"
            )
            # Case A: ì¤‘ë³µ -> ê¸°ì¡´ Masterì— ì—°ê²°
            result_dto = await self.registration_service.link_job_post(
                job_master_id=existing_job_master_id,
                url=url,
                extracted_data=normalized_data,
                fingerprint=fingerprint,
            )
            result = await self._map_to_response_from_dto(result_dto, is_existing=True)
        else:
            logger.info("ğŸ†• No duplicate found. Proceeding to create new JobMaster.")
            # Case B: ì‹ ê·œ -> ìƒˆë¡œ ìƒì„±
            result_dto = await self.registration_service.register_new_job_master(
                url=url,
                extracted_data=normalized_data,
                fingerprint=fingerprint,
                company_id=company_id,
                skill_ids=skill_ids,
            )
            result = await self._map_to_response_from_dto(result_dto, is_existing=False)

        # Transaction Commit at Service Layer End
        await self.session.commit()

        return result

    async def _crawl_content(self, url: str) -> str:
        """URLì—ì„œ í…ìŠ¤íŠ¸ ì½˜í…ì¸ ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤ (ë¹„ë™ê¸° ì²˜ë¦¬)."""
        logger.info("ğŸ†• New URL detected. Proceeding to Crawling...")
        try:
            crawler = CrawlerFactory.get_crawler(url)

            # PlaywrightëŠ” ë¸”ë¡œí‚¹ I/Oì´ë¯€ë¡œ ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
            raw_text = await asyncio.to_thread(crawler.fetch, url)

            if not raw_text or len(raw_text) < 50:
                raise ValueError("Crawled content is empty or too short.")

            logger.info(f"âœ… Crawling successful. Length: {len(raw_text)} chars")
            return raw_text
        except ValueError as e:
            logger.error(f"âŒ Crawling validation failed: {e}")
            raise
        except Exception as e:
            logger.error(f"âŒ Crawling failed: {e}", exc_info=True)
            raise RuntimeError(f"Crawling failed: {e}") from e

    async def _extract_data(self, raw_text: str):
        """LLMì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ì—ì„œ êµ¬ì¡°í™”ëœ ë°ì´í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        try:
            extracted_data = await self.extractor.extract(raw_text)
            if not extracted_data:
                raise RuntimeError("LLM Extraction returned empty result")
            return extracted_data
        except Exception as e:
            logger.error(f"âŒ LLM extraction failed: {e}", exc_info=True)
            if "API" in str(e) or "OpenAI" in str(e):
                raise RuntimeError(f"OpenAI API error: {e}") from e
            raise RuntimeError(f"Data extraction failed: {e}") from e

    async def _handle_content_duplicate(
        self, url: str, extracted_data: Any, fingerprint: str
    ) -> Optional[JobPostingAnalyzeResponse]:
        """ë‚´ìš© ê¸°ë°˜ ì¤‘ë³µ(Fingerprint)ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  ì²˜ë¦¬í•©ë‹ˆë‹¤. (Delegated to DuplicateChecker)"""

        # DuplicateCheckerë¥¼ í†µí•´ Fingerprint ì¤‘ë³µ í™•ì¸ (Relations í¬í•¨ëœ DTO ë°˜í™˜)
        result = await self.duplicate_checker.check_existing_post_by_fingerprint(
            fingerprint
        )

        if result:
            logger.info("â™»ï¸ Content Duplicate detected. Linking to existing Master.")

            # ìƒˆ JobPost ìƒì„± ë° ì—°ê²° (RegistrationService ìœ„ì„)
            duplicate_dto = await self.registration_service.link_job_post(
                job_master_id=result.job_master.job_master_id,
                url=url,
                extracted_data=extracted_data,
                fingerprint=fingerprint,
            )

            # ì¤‘ë³µ ì²˜ë¦¬(ì—°ê²°)ë„ DB ì“°ê¸° ì‘ì—…(JobPost ìƒì„±)ì´ í¬í•¨ë˜ë¯€ë¡œ ì»¤ë°‹ í•„ìš”
            await self.session.commit()

            return await self._map_to_response_from_dto(duplicate_dto, is_existing=True)

        return None

    async def _map_to_response_from_dto(
        self, dto: JobPostingWithRelations, is_existing: bool
    ) -> JobPostingAnalyzeResponse:
        """DTO -> Pydantic Response ë³€í™˜ (JobMaster ê¸°ì¤€, ìµœì í™”)"""

        from shared.schema.job_posting import RecruitmentPeriod

        # DTOì—ì„œ ëª¨ë“  ë°ì´í„° ì¶”ì¶œ (ì¶”ê°€ ì¿¼ë¦¬ ì—†ìŒ)
        job_master = dto.job_master
        company = dto.company
        job_post = dto.job_post

        # RecruitmentPeriod ê°ì²´ ìƒì„±
        recruitment_period = RecruitmentPeriod(
            start_date=job_master.start_date, end_date=job_master.end_date
        )

        return JobPostingAnalyzeResponse(
            job_posting_id=job_post.job_post_id,
            is_existing=is_existing,
            company_name=company.name,
            job_title=job_master.job_title,
            main_responsibilities=(
                job_master.main_tasks if isinstance(job_master.main_tasks, list) else []
            ),
            required_skills=dto.skills,
            recruitment_status=job_master.status,
            recruitment_period=recruitment_period,
            ai_summary=job_master.ai_summary or "",
        )

    async def delete_job_posting(self, job_posting_id: int) -> int:
        """
        Job Posting IDë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê´€ë ¨ ë°ì´í„°(JobMaster, JobPost, Skills, Vector)ë¥¼ ì¼ê´„ ì‚­ì œí•©ë‹ˆë‹¤.
        (Hard Delete)
        """
        logger.info(f"ğŸ—‘ï¸ Deleting Job Posting ID: {job_posting_id}")

        # 1. JobPost ì¡°íšŒ -> JobMaster ID í™•ë³´
        job_post = await self.job_post_repo.find_by_id(job_posting_id)
        if not job_post:
            logger.warning(f"JobPost {job_posting_id} not found.")
            return None

        master_id = job_post.job_master_id
        logger.info(f"Trace JobMaster ID: {master_id}")

        try:
            # # 2. RDB ë°ì´í„° ì‚­ì œ (ìˆœì„œ ì¤‘ìš”: Child -> Parent)

            # # 2-1. Skill ì—°ê²° ì‚­ì œ (JobMasterSkill)
            # await self.skill_repo.delete_job_master_skills(master_id)

            # # 2-2. JobPosts ì‚­ì œ (í•´ë‹¹ ë§ˆìŠ¤í„°ì— ì—°ê²°ëœ ëª¨ë“  ê³µê³ )
            # await self.job_post_repo.delete_by_master_id(master_id)

            # # 2-3. JobMaster ì‚­ì œ
            # await self.job_master_repo.delete(master_id)

            # # 3. Vector DB ì‚­ì œ
            # await self.job_vector_repo.delete_jobs_by_master_id(master_id)

            # # 4. Commit
            # await self.session.commit()

            # logger.info(f"âœ… Successfully deleted JobMaster {master_id} and related data.")

            return job_posting_id

        except Exception as e:
            await self.session.rollback()
            logger.error(f"âŒ Failed to delete job posting: {e}", exc_info=True)
            raise
