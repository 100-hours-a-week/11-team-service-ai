import logging
from shared.db.connection import get_db
from .infrastructure.persistence.job_repository import SqlAlchemyJobRepository
from .infrastructure.persistence.doc_repository import SqlAlchemyDocRepository
from shared.config import settings
from .infrastructure.adapters.llm.openai_agent import LLMAnalyst
from .infrastructure.adapters.llm.mock_agent import MockAnalyst
from .domain.interface.adapter_interfaces import AnalystAgent
from .infrastructure.adapters.storage.s3_storage import S3FileStorage
from .infrastructure.adapters.parser.pdf_extractor import PyPdfExtractor
from .application.services.analyzer import ApplicationAnalyzer
from shared.schema.applicant import EvaluateRequest, EvaluateResponse
from langchain_core.language_models import BaseChatModel

logger = logging.getLogger(__name__)


async def run_pipeline(request: EvaluateRequest) -> EvaluateResponse:
    """
    ì§€ì›ì í‰ê°€ íŒŒì´í”„ë¼ì¸ì˜ ë©”ì¸ ì§„ì…ì  (Async Entrypoint)
    ì™¸ë¶€(API Router)ì—ì„œ í˜¸ì¶œí•  ë•Œ ì´ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    # DB ì„¸ì…˜ ë¼ì´í”„ì‚¬ì´í´ ê´€ë¦¬ (Async Generator)
    async for db_session in get_db():
        # 1. Infrastructure Layerì˜ êµ¬í˜„ì²´ ìƒì„± (Dependencies)
        job_repo = SqlAlchemyJobRepository(db_session)
        doc_repo = SqlAlchemyDocRepository(db_session)

        file_storage = S3FileStorage()
        extractor = PyPdfExtractor()

        # External Clients (DI) & Mock Selection
        # use_mock í”„ë¡œí¼í‹°ê°€ ìˆë‹¤ë©´ ê·¸ê²ƒì„ ì‚¬ìš© (dev profile ì²´í¬ í¬í•¨ë¨)
        # ë˜ëŠ” settings.USE_MOCKì„ ì§ì ‘ ì‚¬ìš©í•´ë„ ë¨
        agent: AnalystAgent
        if getattr(settings, "use_mock", False):
            agent = MockAnalyst()
        else:
            llm_model: BaseChatModel
            if getattr(settings, "LLM_PROVIDER", "openai") == "gemini":
                from langchain_google_genai import ChatGoogleGenerativeAI

                model = getattr(settings, "GOOGLE_MODEL", "gemini-3-flash-preview")
                logger.info(f"ğŸ¤– Initializing LLMJobExtractor with gemini ({model})")

                llm_model = ChatGoogleGenerativeAI(
                    model=model,
                    google_api_key=settings.GOOGLE_API_KEY,
                    temperature=0,
                )
            else:
                from langchain_openai import ChatOpenAI
                from pydantic import SecretStr

                model = getattr(settings, "OPENAI_MODEL", "gpt-4o-mini")
                logger.info(f"ğŸ¤– Initializing LLMJobExtractor with OpenAI ({model})")

                llm_model = ChatOpenAI(
                    model=model,
                    temperature=0,
                    api_key=(
                        SecretStr(settings.OPENAI_API_KEY)
                        if settings.OPENAI_API_KEY
                        else None
                    ),
                    model_kwargs={"response_format": {"type": "json_object"}},
                )
            agent = LLMAnalyst(llm=llm_model)

        # 2. Application Layer ì„œë¹„ìŠ¤ì— ì˜ì¡´ì„± ì£¼ì… (Wiring)
        analyzer = ApplicationAnalyzer(
            job_repo=job_repo,
            doc_repo=doc_repo,
            file_storage=file_storage,
            extractor=extractor,
            agent=agent,
        )

        # 3. ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ì‹¤í–‰ (Async)
        # ì„œë¹„ìŠ¤ ê³„ì¸µì˜ run ë©”ì„œë“œë„ asyncì—¬ì•¼ í•¨
        result = await analyzer.run(int(request.user_id), int(request.job_posting_id))

        # 4. íŠ¸ëœì­ì…˜ ì»¤ë°‹ (ì €ì¥ì†Œ ë³€ê²½ì‚¬í•­ ë°˜ì˜)
        await db_session.commit()

        return result

    raise RuntimeError("Failed to obtain database session")
