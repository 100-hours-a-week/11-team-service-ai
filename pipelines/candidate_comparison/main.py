import logging
from shared.db.connection import get_db
from shared.config import settings
from shared.schema.applicant import CompareRequest, CompareResponse

# Domain Interface
from .domain.interface.adapter_interfaces import ComparisonAnalyzer

# Application Service
from .application.services.comparison_use_case import ComparisonUseCase

# Infrastructure (Persistence)
from .infrastructure.persistence.candidate_repository import (
    SqlAlchemyCandidateRepository,
)
from .infrastructure.persistence.job_repository import SqlAlchemyJobRepository

# Infrastructure (Adapters)
from .infrastructure.adapters.llm.mock_agent import MockComparisonAnalyzer
from .infrastructure.adapters.llm.ai_agent.graph import LLMAnalyst

logger = logging.getLogger(__name__)


async def run_pipeline(request: CompareRequest) -> CompareResponse:
    """
    ì§€ì›ìž ë¹„êµ íŒŒì´í”„ë¼ì¸ì˜ ë©”ì¸ ì§„ìž…ì  (Async Entrypoint)
    ì™¸ë¶€(API Router ë˜ëŠ” pipeline_bridge)ì—ì„œ í˜¸ì¶œí•  ë•Œ ì´ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

    Args:
        request: CompareRequest
            - job_posting_id: ë¹„êµ ê¸°ì¤€ ê³µê³  ID
            - user_id: ë‚´ ì§€ì›ìž ID
            - competitor: ë¹„êµ ëŒ€ìƒ ì§€ì›ìž ID

    Returns:
        CompareResponse: ë¹„êµ ê²°ê³¼ (comparison_metrics, strengths_report, weaknesses_report)
    """
    logger.info(
        f"ðŸš€ [Pipeline Start] Comparing user {request.user_id} vs {request.competitor} "
        f"for job {request.job_posting_id}"
    )

    use_case = None

    # --- [Step 1] ë°ì´í„° ì¤€ë¹„ (ì„¸ì…˜ 1) ---
    async for db_session in get_db():
        use_case = _create_use_case(db_session)
        my_candidate, competitor_candidate, job_info = (
            await use_case.prepare_comparison_data(
                my_candidate_id=str(request.user_id),
                competitor_candidate_id=str(request.competitor),
                job_posting_id=str(request.job_posting_id),
            )
        )
        await db_session.commit()
        break

    if not use_case:
        raise RuntimeError("Failed to obtain database session")

    # --- [Step 2] AI ë¶„ì„ (DB ì—°ê²° ë¶ˆí•„ìš”) ---
    strengths, weaknesses = await use_case.run_ai_comparison(
        my_candidate=my_candidate,
        competitor_candidate=competitor_candidate,
        job_info=job_info,
    )

    # --- [Step 3] ìµœì¢… ì‘ë‹µ í¬ë§·íŒ… ë° ë°˜í™˜ (DB ì—°ê²° ë¶ˆí•„ìš”) ---
    result = use_case.format_comparison_response(
        my_candidate=my_candidate,
        competitor_candidate=competitor_candidate,
        strengths=strengths,
        weaknesses=weaknesses,
    )

    logger.info(
        f"âœ¨ [Pipeline Complete] Comparison finished for user {request.user_id}"
    )
    return result


def _create_use_case(db_session) -> ComparisonUseCase:
    candidate_repo = SqlAlchemyCandidateRepository(db_session)
    job_repo = SqlAlchemyJobRepository(db_session)

    analyzer: ComparisonAnalyzer

    if getattr(settings, "use_mock", False):
        logger.info("ðŸ¤– Using Mock Comparison Analyzer")
        analyzer = MockComparisonAnalyzer()
    else:
        llm_provider = getattr(settings, "LLM_PROVIDER", "openai")
        if llm_provider == "gemini":
            model_name = getattr(settings, "GOOGLE_MODEL", "gemini-3-flash-preview")
        elif llm_provider == "vllm":
            model_name = getattr(settings, "VLLM_MODEL", "Qwen/Qwen3-32B-FP8")
        else:
            model_name = getattr(settings, "OPENAI_MODEL", "gpt-4o-mini")

        analyzer = LLMAnalyst(model_name=model_name, model_provider=llm_provider)

    return ComparisonUseCase(
        candidate_repo=candidate_repo,
        job_repo=job_repo,
        ai_analyzer=analyzer,
    )
